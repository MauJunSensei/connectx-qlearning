{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2En99A7pW24n"
      },
      "source": [
        "# Install kaggle-environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-ty1vRAWW24o",
        "outputId": "44a7b65a-bcdc-4abc-eaaf-ee584df8d86b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# 1. Enable Internet in the Kernel (Settings side pane)\n",
        "\n",
        "# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed).\n",
        "# !curl -X PURGE https://pypi.org/simple/kaggle-environments\n",
        "\n",
        "# ConnectX environment was defined in v0.1.6\n",
        "%pip install -q \"kaggle-environments>=0.1.6\"\n",
        "%pip install -q numpy\n",
        "%pip install -q tqdm\n",
        "%pip install -q stable-baselines3\n",
        "%pip install -q kaggle-environments stable-baselines3 gymnasium numpy torch tqdm matplotlib\n",
        "%pip install -q sb3-contrib\n",
        "%pip install -q lmdb\n",
        "%pip install -q gym tensorboard\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRbnxgpMW24p"
      },
      "source": [
        "# Create ConnectX Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AGWCA6uYW24p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+---+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+---+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from kaggle_environments import evaluate, make, utils\n",
        "\n",
        "env = make(\"connectx\", debug=True)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA1TJTltW24p"
      },
      "source": [
        "# Enviroment wrapper\n",
        "\n",
        "To create the submission, an agent function should be fully encapsulated (no external dependencies).  \n",
        "\n",
        "When your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch (1.3.1, cpu only), and more may be added later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C1UNhBXM9Oql"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "from kaggle_environments import make\n",
        "\n",
        "\n",
        "\n",
        "class ConnectXGym(gym.Env):\n",
        "    \"\"\"\n",
        "    A Gym-compatible wrapper for Kaggle's ConnectX environment.\n",
        "    Allows training with Stable Baselines 3's algorithms.\n",
        "    \"\"\"\n",
        "    def __init__(self, opponent=\"random\"):\n",
        "        super().__init__()\n",
        "        # Initialize the Kaggle ConnectX environment\n",
        "        self.env = make(\"connectx\", debug=True)\n",
        "        # Create a training helper that automatically handles the opponent's moves\n",
        "        self.trainer = self.env.train([None, opponent])\n",
        "\n",
        "        # Save the environment configuration\n",
        "        self.config = self.env.configuration\n",
        "\n",
        "        # Define Gym spaces\n",
        "        self.action_space = spaces.Discrete(self.config.columns)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0,\n",
        "            high=2,\n",
        "            shape=(self.config.rows * self.config.columns,),\n",
        "            dtype=np.int8\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Resets the environment to an initial state and returns an initial observation.\n",
        "        \"\"\"\n",
        "        obs_dict = self.trainer.reset()\n",
        "        obs = np.array(obs_dict[\"board\"], dtype=np.int8)\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action: int):\n",
        "        # Check if action is invalid (the top cell is not empty)\n",
        "        current_obs = self.env.state[0][\"observation\"]  # returns a Struct\n",
        "        board = current_obs[\"board\"]\n",
        "\n",
        "        # If invalid, penalize and end\n",
        "        if board[action] != 0:  # Invalid move\n",
        "            obs = np.array(board, dtype=np.int8)\n",
        "            reward = -1.0  # Small penalty instead of hard termination\n",
        "            \"\"\"\n",
        "            done = False   # Allow continuation\n",
        "\n",
        "            \"\"\"\n",
        "            done = True  # Ends the episode immediately (game over)\n",
        "            truncated = False\n",
        "            return np.array(board, dtype=np.int8), reward, done, truncated, {\"invalid_action\": True}\n",
        "           \n",
        "           \n",
        "        # Otherwise, proceed\n",
        "        obs_dict, reward, done, info = self.trainer.step(int(action))\n",
        "\n",
        "        if reward is None:\n",
        "            reward = 0.0\n",
        "        else:\n",
        "            reward = float(reward)\n",
        "\n",
        "        obs = np.array(obs_dict[\"board\"], dtype=np.int8)\n",
        "        truncated = False\n",
        "        return obs, reward, done, truncated, info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VhKDZn-W24q"
      },
      "source": [
        "# PPO Agent training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1El2iio8W24q",
        "outputId": "093f30ea-a0f0-48d6-9f11-d8d9c6336a44"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv  # or DummyVecEnv\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# 1) Create environment functions\n",
        "def make_env_random():\n",
        "    def _init():\n",
        "        return ConnectXGym(opponent=\"random\")\n",
        "    return _init\n",
        "\n",
        "def make_env_negamax():\n",
        "    def _init():\n",
        "        return ConnectXGym(opponent=\"negamax\")\n",
        "    return _init\n",
        "\n",
        "# 2) 2 envs with random, 2 envs with negamax => total n_envs=4\n",
        "env_fns = [make_env_random(), make_env_random(), make_env_negamax(), make_env_negamax()]\n",
        "\n",
        "vec_env = SubprocVecEnv(env_fns)  # or DummyVecEnv(env_fns)\n",
        "\n",
        "# 3) Instantiate the PPO model\n",
        "\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    vec_env,\n",
        "    verbose=0,             # Disable logging\n",
        "    tensorboard_log=\"./ppo_tensorboard/\",\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    gamma=0.99,           # Lower discount factor (default: 0.99)\n",
        "    ent_coef=0.01,         # Higher entropy for more exploration (default: 0.01)\n",
        "    learning_rate=3e-4,   # Keep default for now\n",
        "    n_steps=2048,         # Keep default\n",
        ")\n",
        "\n",
        "\n",
        "print(\"ðŸš€ Starting PPO training...\")\n",
        "\n",
        "# 4) Manual Loop: train in intervals of 10k steps, then evaluate\n",
        "from kaggle_environments import evaluate\n",
        "import numpy as np\n",
        "\n",
        "def ppo_agent(obs, config):\n",
        "    board_array = np.array(obs[\"board\"], dtype=np.int8)\n",
        "    action, _ = model.predict(board_array, deterministic=True)\n",
        "    return int(action)\n",
        "\n",
        "def mean_reward(rewards):\n",
        "    valid_rewards = [(r[0] if r[0] else 0.0, r[1] if r[1] else 0.0)\n",
        "                     for r in rewards]\n",
        "    return sum(r[0] for r in valid_rewards) / float(len(valid_rewards))\n",
        "\n",
        "TOTAL_STEPS = 250000\n",
        "EVAL_INTERVAL = 10000\n",
        "current_steps = 0\n",
        "\n",
        "\n",
        "model.learn(total_timesteps=TOTAL_STEPS, reset_num_timesteps=False)\n",
        "\n",
        "while current_steps < TOTAL_STEPS:\n",
        "    model.learn(total_timesteps=EVAL_INTERVAL, reset_num_timesteps=False)\n",
        "    current_steps += EVAL_INTERVAL\n",
        "\n",
        "    # Evaluate vs random\n",
        "    eval_rand = evaluate(\"connectx\", [ppo_agent, \"random\"], num_episodes=500)\n",
        "    mr_rand = mean_reward(eval_rand)\n",
        "\n",
        "    # Evaluate vs negamax\n",
        "    eval_nega = evaluate(\"connectx\", [ppo_agent, \"negamax\"], num_episodes=500)\n",
        "    mr_nega = mean_reward(eval_nega)\n",
        "\n",
        "    print(f\"Steps={current_steps}: vs. Random={mr_rand:.5f}, vs. Negamax={mr_nega:.5f}\")\n",
        "\n",
        "model.save(\"ppo_model\")\n",
        "\n",
        "print(\"âœ… PPO Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfyqm5u8W24q"
      },
      "source": [
        "# DQN Agent training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBX1plehW24q",
        "outputId": "9967419a-ffc0-45ea-d0be-a24232fa6dd3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv  # or DummyVecEnv\n",
        "\n",
        "# 1) Create environment functions\n",
        "def make_env_random():\n",
        "    def _init():\n",
        "        return ConnectXGym(opponent=\"random\")\n",
        "    return _init\n",
        "\n",
        "def make_env_negamax():\n",
        "    def _init():\n",
        "        return ConnectXGym(opponent=\"negamax\")\n",
        "    return _init\n",
        "\n",
        "# 2) 2 envs with random, 2 envs with negamax => total n_envs=4\n",
        "env_fns = [make_env_random(), make_env_random(), make_env_negamax(), make_env_negamax()]\n",
        "vec_env = SubprocVecEnv(env_fns)  # or DummyVecEnv(env_fns)\n",
        "\n",
        "# 3) Instantiate the DQN model\n",
        "model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    vec_env,\n",
        "    learning_rate=1e-3,\n",
        "    buffer_size=100_000,\n",
        "    batch_size=64,\n",
        "    gamma=0.95,\n",
        "    exploration_fraction=0.2,\n",
        "    exploration_final_eps=0.01,\n",
        "    target_update_interval=500,\n",
        "    train_freq=4,\n",
        "    verbose=0,  # Disable logging\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting DQN training...\")\n",
        "\n",
        "# 4) Manual Loop: train in intervals of 10k steps, then evaluate\n",
        "from kaggle_environments import evaluate\n",
        "import numpy as np\n",
        "\n",
        "def dqn_agent(obs, config):\n",
        "    board_array = np.array(obs[\"board\"], dtype=np.int8)\n",
        "    action, _ = model.predict(board_array, deterministic=True)\n",
        "    return int(action)\n",
        "\n",
        "def mean_reward(rewards):\n",
        "    valid_rewards = [(r[0] if r[0] else 0.0, r[1] if r[1] else 0.0)\n",
        "                     for r in rewards]\n",
        "    return sum(r[0] for r in valid_rewards) / float(len(valid_rewards))\n",
        "\n",
        "TOTAL_STEPS = 250000  \n",
        "EVAL_INTERVAL = 10000\n",
        "current_steps = 0\n",
        "\n",
        "while current_steps < TOTAL_STEPS:\n",
        "    model.learn(total_timesteps=EVAL_INTERVAL, reset_num_timesteps=False)\n",
        "    current_steps += EVAL_INTERVAL\n",
        "\n",
        "    eval_rand = evaluate(\"connectx\", [dqn_agent, \"random\"], num_episodes=500)\n",
        "    mr_rand = mean_reward(eval_rand)\n",
        "\n",
        "    eval_nega = evaluate(\"connectx\", [dqn_agent, \"negamax\"], num_episodes=500)\n",
        "    mr_nega = mean_reward(eval_nega)\n",
        "\n",
        "    print(f\"Steps={current_steps}: vs. Random={mr_rand:.5f}, vs. Negamax={mr_nega:.5f}\")\n",
        "\n",
        "# Save the DQN model\n",
        "model.save(\"dqn_model\")\n",
        "\n",
        "print(\"âœ… DQN Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# # Define path to main.py\n",
        "# main_script = os.path.join(os.getcwd(), \"main.py\")\n",
        "\n",
        "# # Run the script to train Q-learning\n",
        "# print(\"ðŸš€ Running Q-learning training script...\")\n",
        "# os.system(f\"python {main_script}\")  # This executes main.py like a command-line script\n",
        "# print(\"âœ… Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q-Learning Training Results:\n",
        "\n",
        "```\n",
        "Episode 20000 - Random: 0.551, Negamax: 0.02\n",
        "Episode 30000 - Random: 0.564, Negamax: 0.017\n",
        "Episode 40000 - Random: 0.589, Negamax: 0.022\n",
        "Episode 50000 - Random: 0.579, Negamax: 0.033\n",
        "Episode 60000 - Random: 0.576, Negamax: 0.023\n",
        "Episode 70000 - Random: 0.559, Negamax: 0.023\n",
        "Episode 80000 - Random: 0.577, Negamax: 0.022\n",
        "Episode 90000 - Random: 0.58, Negamax: 0.02\n",
        "Episode 100000 - Random: 0.55, Negamax: 0.016\n",
        "Episode 110000 - Random: 0.553, Negamax: 0.015\n",
        "Episode 120000 - Random: 0.589, Negamax: 0.018\n",
        "Episode 130000 - Random: 0.56, Negamax: 0.02\n",
        "Episode 140000 - Random: 0.571, Negamax: 0.02\n",
        "Episode 150000 - Random: 0.546, Negamax: 0.013\n",
        "Episode 160000 - Random: 0.568, Negamax: 0.026\n",
        "Episode 170000 - Random: 0.586, Negamax: 0.031\n",
        "Episode 180000 - Random: 0.563, Negamax: 0.027\n",
        "Episode 190000 - Random: 0.575, Negamax: 0.021\n",
        "Episode 200000 - Random: 0.545, Negamax: 0.024\n",
        "Episode 210000 - Random: 0.589, Negamax: 0.023\n",
        "Episode 220000 - Random: 0.565, Negamax: 0.019\n",
        "Episode 230000 - Random: 0.531, Negamax: 0.017\n",
        "Episode 240000 - Random: 0.569, Negamax: 0.025\n",
        "Episode 250000 - Random: 0.535, Negamax: 0.027\n",
        "Q-table size: 176014\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Extracted qtable_backup.zip into qtable.lmdb\n",
            "âœ… Q-learning model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from kaggle_environments import evaluate\n",
        "\n",
        "# Add src/ directory to Python path\n",
        "sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
        "\n",
        "# Import Q-learning components\n",
        "from qtable import QTable\n",
        "from connectx import ConnectX\n",
        "import zipfile\n",
        "\n",
        "# Initialize ConnectX environment\n",
        "env = ConnectX()\n",
        "\n",
        "# Define the path to the zip file and the extraction directory\n",
        "zip_file_path = \"qtable_backup.zip\"\n",
        "extract_dir = \"qtable.lmdb\"\n",
        "\n",
        "# Check if the extraction directory already exists\n",
        "if not os.path.exists(extract_dir):\n",
        "    # If not, unzip the file into the directory\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"âœ… Extracted {zip_file_path} into {extract_dir}\")\n",
        "else:\n",
        "    print(f\"â„¹ï¸ Directory {extract_dir} already exists. Skipping extraction.\")\n",
        "\n",
        "q_table = QTable(action_space=env.action_space.n)\n",
        "\n",
        "print(\"âœ… Q-learning model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models battle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”µ DQN vs PPO Mean Reward: 1.00000\n",
            "ðŸ”µ PPO vs DQN Mean Reward: 1.00000\n",
            "ðŸ”µ Q-Learning vs PPO Mean Reward: 0.14800\n",
            "ðŸ”µ PPO vs Q-Learning Mean Reward: 0.96400\n",
            "ðŸ”µ DQN vs Q-Learning Mean Reward: 0.97600\n",
            "ðŸ”µ Q-Learning vs DQN Mean Reward: 0.41700\n"
          ]
        }
      ],
      "source": [
        "from random import choice\n",
        "from kaggle_environments import evaluate, make\n",
        "from stable_baselines3 import PPO, DQN\n",
        "\n",
        "def mean_reward(rewards):\n",
        "    valid_rewards = [(r[0] if r[0] else 0.0, r[1] if r[1] else 0.0)\n",
        "                     for r in rewards]\n",
        "    return sum(r[0] for r in valid_rewards) / float(len(valid_rewards))\n",
        "\n",
        "ppo_model = PPO.load(\"ppo_model\")\n",
        "dqn_model = DQN.load(\"dqn_model\")\n",
        "q_model = q_table.get_table()\n",
        "\n",
        "# Define agent functions using loaded models\n",
        "def ppo_agent(obs, config):\n",
        "    board_array = np.array(obs[\"board\"], dtype=np.int8)\n",
        "    action, _ = ppo_model.predict(board_array, deterministic=True)\n",
        "    return int(action)\n",
        "\n",
        "def dqn_agent(obs, config):\n",
        "    board_array = np.array(obs[\"board\"], dtype=np.int8)\n",
        "    action, _ = dqn_model.predict(board_array, deterministic=True)\n",
        "    return int(action)\n",
        "\n",
        "def q_agent(obs, conf):\n",
        "    state_key = hex(int(''.join(map(str, obs.board + [obs.mark])), 3))[2:] # convert the state to a unique key\n",
        "    return q_model.get(state_key, choice([c for c in range(conf.columns) if obs.board[c] == 0])) # get the action from the Q-table\n",
        "\n",
        "# Evaluate PPO vs DQN\n",
        "eval_results = evaluate(\"connectx\", [dqn_agent, ppo_agent], num_episodes=500)\n",
        "mr_dqn_vs_ppo = mean_reward(eval_results)\n",
        "\n",
        "eval_results = evaluate(\"connectx\", [ppo_agent, dqn_agent], num_episodes=500)\n",
        "mr_ppo_vs_dqn = mean_reward(eval_results)\n",
        "\n",
        "# Evaluate Q-Learning vs PPO\n",
        "eval_results = evaluate(\"connectx\", [q_agent, ppo_agent], num_episodes=500)\n",
        "mr_qlearn_vs_ppo = mean_reward(eval_results)\n",
        "\n",
        "eval_results = evaluate(\"connectx\", [ppo_agent, q_agent], num_episodes=500)\n",
        "mr_ppo_vs_qlearn = mean_reward(eval_results)\n",
        "\n",
        "# Evaluate Q-Learning vs DQN\n",
        "eval_results = evaluate(\"connectx\", [dqn_agent, q_agent], num_episodes=500)\n",
        "mr_dqn_vs_qlearn = mean_reward(eval_results)\n",
        "\n",
        "eval_results = evaluate(\"connectx\", [q_agent, dqn_agent], num_episodes=500)\n",
        "mr_qlearn_vs_dqn = mean_reward(eval_results)\n",
        "\n",
        "print(f\"ðŸ”µ DQN vs PPO Mean Reward: {mr_dqn_vs_ppo:.5f}\")\n",
        "print(f\"ðŸ”µ PPO vs DQN Mean Reward: {mr_ppo_vs_dqn:.5f}\")\n",
        "print(f\"ðŸ”µ Q-Learning vs PPO Mean Reward: {mr_qlearn_vs_ppo:.5f}\")\n",
        "print(f\"ðŸ”µ PPO vs Q-Learning Mean Reward: {mr_ppo_vs_qlearn:.5f}\")\n",
        "print(f\"ðŸ”µ DQN vs Q-Learning Mean Reward: {mr_dqn_vs_qlearn:.5f}\")\n",
        "print(f\"ðŸ”µ Q-Learning vs DQN Mean Reward: {mr_qlearn_vs_dqn:.5f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
